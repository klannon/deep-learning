# ---------------------------------------------------------------------
# Trains a neural network on the run_3v data generated by OSU
#
# Authors: Colin Dablain, Matthew Drnevich, Kevin Lannon
# ---------------------------------------------------------------------

from __future__ import print_function

import os
import os.path
import sys

import argparse
import theano

from physics import PHYSICS # in order for this to not give an ImportError, need to
# set PYTHONPATH (see README.md)
from template.terminators import Timeout
from time import time

import pylearn2
import pylearn2.training_algorithms.sgd
import pylearn2.models.mlp as mlp
import pylearn2.train
import pylearn2.space
import pylearn2.termination_criteria

from monitoring import TrainVeil, make_data_slim

def init_train(learningRate, train, test, batchSize, numLayers, nodesPerLayer,
               timeout=None, maxEpochs=None, benchmark=None, saveDir='.', monitorFraction=(0.02, 0.5), *args, **kwargs):
    hostname = os.getenv("HOST", os.getpid()) # So scripts can be run simultaneously on different machines
    if saveDir == '.':
        results_dir = "{1}{0}results{0}".format(os.sep, os.getcwd())
    else:
        results_dir = saveDir if os.path.split(saveDir) is '' else saveDir+os.sep

    if results_dir.split(os.sep)[0] == '.':
        results_dir = os.getcwd() + os.sep + os.sep.join(results_dir.split(os.sep)[1:])

    pwd = ''
    while results_dir.split(os.sep)[0] == '..':
        results_dir = os.sep.join(results_dir.split(os.sep)[1:])
        if pwd == '':
            pwd = os.sep.join(os.getcwd().split(os.sep)[:-1])
        else:
            pwd = os.sep.join(pwd.split(os.sep)[:-1])
    if pwd:
        results_dir = pwd + os.sep + results_dir

    if not os.path.isdir(results_dir):
        os.mkdir(results_dir)
    idpath = "{}{}_time{}".format(results_dir, hostname, time())
    save_path = idpath + '.pkl'

    # Dataset
    pylearn_path = os.environ['PYLEARN2_DATA_PATH']+os.sep
    path_to_train_X, path_to_train_Y = [pylearn_path+f for f in train]
    path_to_test_X, path_to_test_Y = [pylearn_path+f for f in test]

    dataset_train, dataset_test = PHYSICS(), PHYSICS()
    dataset_train.load_from_file(path_to_train_X, path_to_train_Y, benchmark=benchmark, which_set='train')
    dataset_test.load_from_file(path_to_test_X, path_to_test_Y, benchmark=benchmark, which_set='test')

    monitor_train, monitor_test = make_data_slim((dataset_train, dataset_test), monitorFraction)

    nvis = dataset_train.X.shape[1] # number of visible layers

    # Model
    network_layers = []
    count = 1
    while(count <= numLayers):
        network_layers.append(mlp.RectifiedLinear(
            layer_name=('r{}'.format(count)),
            dim=nodesPerLayer,
            istdev=.1))
        count += 1
    # add final layer
    network_layers.append(mlp.Softmax(
        layer_name='y',
        n_classes=2,
        istdev=.001))
    model = pylearn2.models.mlp.MLP(layers=network_layers,
                                     nvis=nvis)

    # Configure when the training will terminate
    if timeout:
        terminator = Timeout(timeout*60)  # Timeout takes an argument in seconds, so timeout is in minutes
    elif maxEpochs:
        terminator = pylearn2.termination_criteria.EpochCounter(max_epochs=maxEpochs)
    else:
        terminator = None

    # Algorithm
    algorithm = pylearn2.training_algorithms.sgd.SGD(
        batch_size=batchSize,
        learning_rate=learningRate,
        monitoring_batches=1,
        monitoring_dataset={'train': monitor_train,
                              'test': monitor_test},
        # update_callbacks=pylearn2.training_algorithms.sgd.ExponentialDecay(
        #     decay_factor=1.0000003, # Decreases by this factor every batch. (1/(1.000001^8000)^100 
        #     min_lr=.000001
        # ),
        termination_criterion=terminator
    )
    # Train
    train = pylearn2.train.Train(dataset=dataset_train,
                                 model=model,
                                 algorithm=algorithm,
                                 save_path=save_path,
                                 save_freq=100)

    TrainVeil(train)

    return train


def train(mytrain, batchSize, timeout, maxEpochs, *args, **kwargs):
    # Execute training loop.
    logfile = os.path.splitext(mytrain.save_path)[0] + '.log'
    print('Using={}'.format(theano.config.device)) # Can use gpus.
    print('Writing to {}'.format(logfile))
    print('Saving to {}'.format(mytrain.save_path))
    sys.stdout = open(logfile, 'w')
    #
    # print statements after here are written to the log file
    #
    print("Opened log file")
    print("Model:")
    print(mytrain.model)
    print("\n\nAlgorithm:")
    print(mytrain.algorithm)
    print("\n\nAdditional Hyperparameters:")
    print("Batch size: {}".format(batchSize))
    print("Maximum Epochs: {}".format(maxEpochs))
    print("Maximum runtime: {} minutes".format(timeout))
    # All of the other  hyperparameters can be deduced from the log file
    mytrain.main_loop()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    ###################################
    ## SET UP COMMAND LINE ARGUMENTS ##
    ###################################

    parser.add_argument("-r", "--learningRate", help="learning rate", type=float, default=0.001)
    parser.add_argument("-b", "--batchSize", help="size of each batch "
                        + "(subset of training set)", type=int, default=32)
    parser.add_argument("-l", "--numLayers",
                        help="number of hidden layers in the network", type=int, default=4)
    parser.add_argument("-e", "--maxEpochs",
                        help="number of epochs to run for", type=int, default=None)
    parser.add_argument("-n", "--nodesPerLayer",
                        help="number of nodes per layer", type=int, default=50)
    parser.add_argument("-t", "--timeout",
                        help="how long it should train in minutes", type=float, default=None)
    parser.add_argument("-mf", "--monitorFraction", help="a two-tuple with the training and testing monitoring percents",
                        default=(0.02, 0.5), type=tuple)
    parser.add_argument("-m", "--benchmark", help="keyword[s] that represent the type of data", default=None)
    parser.add_argument("-s", "--saveDir", help="parent directory to save the results in", default='.')
    parser.add_argument("train", nargs=2, metavar='train_file', help="the <train_X>.npy and <train_Y>.npy files")
    parser.add_argument("test", nargs=2, metavar='test_file', help="the <test_X>.npy and <test_Y>.npy files")
    args = vars(parser.parse_args())

    # maxEpochs is specified in the call to run()
    print("PARAMETERS")
    print("Learning Rate: {}".format(args['learningRate']))
    print("Batch Size: {}".format(args['batchSize']))
    print("Number of Layers: {}".format(args['numLayers']))
    print("Number of Epochs to run for: {}".format(args['maxEpochs']))
    print("Number of nodes per layer: {}".format(args['nodesPerLayer']))
    print("Timeout: {}".format(args['timeout']))
    print("Save Directory: {}{}".format(args['saveDir'], os.linesep))

    ##########################################
    ## INITIALIZE TRAINING OBJECT AND TRAIN ##
    ##########################################

    mytrain = init_train(**args)
    train(mytrain, **args)
